---
title: "NUS-IDS at FinCausal 2021: Dependency Tree in Graph Neural Network for Better Cause-Effect Span Detection"
collection: publications
permalink: /publication/2021-09-15-cause-effect-detection
excerpt: 'Automatic identification of cause-effect spans in financial documents is important for causality modelling and understanding reasons that lead to financial events. To exploit the observation that words are more connected to other words with the same cause-effect type in a dependency tree, we construct useful graph embeddings by incorporating dependency relation features through a graph neural network. Our model builds on a baseline BERT token classifier with Viterbi decoding (Kao et al., 2020), and outperforms this baseline in cross-validation and during the competition. In the official run of FinCausal 2021, we obtained Precision, Recall, and F1 scores of 95.56%, 95.56% and 95.57% that all ranked 1st place, and an Exact Match score of 86.05% which ranked 3rd place.'
date: 2021-09-15
venue: '3rd Financial Narrative Processing Workshop'
paperurl: 'https://aclanthology.org/2021.fnp-1.6'
citation: 'Fiona Anting Tan and See-Kiong Ng. 2021. NUS-IDS at FinCausal 2021: Dependency Tree in Graph Neural Network for Better Cause-Effect Span Detection. In Proceedings of the 3rd Financial Narrative Processing Workshop, pages 37â€“43, Lancaster, United Kingdom. Association for Computational Linguistics.'
---

<img src='../images/posters/FinCausal_SharedTask_FNP_2021_POSTER.png' width=500>


<a href='https://aclanthology.org/2021.fnp-1.6'>Download paper here</a>


<a href='https://github.com/tanfiona/causeeffectdetection'>Visit our Github respository here</a>

Automatic identification of cause-effect spans in financial documents is important for causality modelling and understanding reasons that lead to financial events. To exploit the observation that words are more connected to other words with the same cause-effect type in a dependency tree, we construct useful graph embeddings by incorporating dependency relation features through a graph neural network. Our model builds on a baseline BERT token classifier with Viterbi decoding (Kao et al., 2020), and outperforms this baseline in cross-validation and during the competition. In the official run of FinCausal 2021, we obtained Precision, Recall, and F1 scores of 95.56%, 95.56% and 95.57% that all ranked 1st place, and an Exact Match score of 86.05% which ranked 3rd place.
